\documentclass[
	%a4paper, % Use A4 paper size
	letterpaper, % Use US letter paper size
]{jdf}
\addbibresource{references.bib}
\newcommand{\pcite}[1]{(\cite{#1})}

\usepackage{enumitem}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath, amsthm}
\usepackage{booktabs}
\RequirePackage[colorlinks]{hyperref}
\usepackage[lined,boxed,linesnumbered,commentsnumbered]{algorithm2e}
\usepackage{xcolor}
\usepackage{listings}
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{red},
  keywordstyle=\color{blue}
}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

% Commands
\newenvironment{solution}
  {\begin{proof}[Solution]}
  {\end{proof}}

\title{CSE6250: Big Data Analytics in Healthcare \\ Project Proposal}
\author{Nick Jaramishian (njaramishian3) and William Luna (wluna6)}

\begin{document}

\maketitle

\section{Papers}

\subsection{Explainable automated coding of clinical notes using hierarchical label-wise attention networks and label embedding initialisation}
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
\item \textbf{Index}: 9, \textbf{Venue}: Journal of Biomedical Informatics, \textbf{Citation}: \pcite{explainable_note_coding}
\item \textbf{Authors}: Hang Dong, Víctor Suarez-Paniagua, William Whiteley, Honghan Wu 
\item \textbf{Task}: Labeling clinical notes with medical codes based on clinical conditions described in notes using an automated process based on a DL model.
\item \textbf{Innovation}: The authors develop a new neural network architecture (Hierarchical Label-Wise Attention Network "HLAN") to provide explainable labeling of clinical notes via attention weights. 
\item \textbf{Advantages / Disadvantages}: The largest advantage of their approach is that the HLAN is able to quanitfy which words in the clinical notes are the most important to the codes assigned to the notes, providing insight into the model's representation of the notes (as opposed to previous black-box methods). One disadvantage is that the HLAN performs slightly worse than SOTA black-box models on certain tasks. 
\item \textbf{Data}: Accessible via (MIMIC-III), \textbf{Code}: Provided by authors \href{https://github.com/acadTags/Explainable-Automated-Medical-Coding}{on Github}.
\end{itemize}

\subsection{Improving Medical Code Prediction from Clinical Text via Incorporating Online Knowledge Sources}
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
\item \textbf{Index}: 44, \textbf{Venue}: 2019 ACM Worldwide Conference, \textbf{Citation}: \pcite{improving_code_prediction}
\item \textbf{Authors}: Tian Bai & Slobodan Vucetic
\item \textbf{Task}: Accurately predicting ICD codes from clinical notes via several deep learning models–RNN and CNN with and without an attention mechanism.
\item \textbf{Innovation}: Supplementing the models with Wikipedia's descriptions of disease codes to improve accuracy.
\item \textbf{Advantages / Disadvantages}: The advantage of this approach is that it reliably improves model accuracy by enriching the model with external data. However, at time of publication not all ICD codes had descriptions on Wikipedia, limiting how well the approach works without adding additional data sources for the remaining codes.
\item \textbf{Data}: Accessible via Mimic-III, \textbf{Code}: provided by authors \href{https://github.com/tiantiantu/KSI}{on GitHub}
\end{itemize}

\subsection{EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records}
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
\item \textbf{Index}: 90, \textbf{Venue}: arXiv, \textbf{Citation}: \pcite{lee2023ehrsqlpracticaltexttosqlbenchmark}
\item \textbf{Authors}: Gyubok Lee, Hyeonji Hwang, Seongsu Bae, Yeonsu Kwon, Woncheol Shin, Seongjun Yang, Minjoon Seo, Jongyeup Kim, Edward Choi
\item \textbf{Task}: Translation of text/speech from medical practitioners into SQL to unlock the potential of dynamically querying EHR databases. 
\item \textbf{Innovation}: A new text-to-SQL benchmark for EHR data (EHRSQL) is created from a poll of members of a university hospital. The authors also develop a methodology to test language models against the benchmark. 
\item \textbf{Advantages / Disadvantages}: One advantage of their benchmark evaluation methodology is that it allows the model to discern whether a text question posed is answerable or unanswerable. One of the major disadvantages of this benchmark is that it was created from responses from a single Korean university hospital -- a wider range of hospitals and practitioners would likely provide a more generalized and robust benchmark. Given that the paper was created in 2022, the authors use a now outdated LLM (T5), where newer LLMs may be more capable on text-to-SQL tasks. 
\item \textbf{Data}: Accessible via Mimic-III and eICU. \href{https://physionet.org/content/eicu-crd/2.0/}{Users must complete a course for eICU}.
\item \textbf{Code}: Provided with benchmark data \href{https://github.com/glee4810/EHRSQL}{on Github}.
\end{itemize}

\section{Paper Selection}
Our team has \textbf{decided to replicate paper} 1.2 \pcite{improving_code_prediction}. \textbf{We chose this paper because} its use of Wikipedia data not only serves as a precursor to Retrieval-Augmented Generation(RAG), which has become a staple in SOTA LLM development, but also for its comparison of the same RNN model with and without an attention mechanism. In our reproduction study, \textbf{we plan to verify the specific hypothesis} that providing a model Wikipedia's ICD (International Classification of Diseases) documentation improves that model's ability to predict medical codes from unstructured clinical notes. We may also \textbf{expand on the paper's ablation study} by performing it on additional baseline models besides CAML (Convolutional Attention).

We feel confident that \textbf{the data is accessible} via MIMIC-III and Wikipedia, that the Python environment used to run the models are clearly specified in the code base, \textbf{and that computation will not exceed what our team can reasonably acquire} via populare cloud providers.

\printbibliography{}

\end{document}
